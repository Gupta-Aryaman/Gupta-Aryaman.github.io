[{"content":"\nIntroduction Medical prescriptions, often scribbled in hurried handwriting, pose a significant challenge when attempting to extract valuable information.\nAutomating this process requires a robust combination of Optical Character Recognition (OCR) and Natural Language Processing (NLP) tools to accurately identify entities like medication names, dosages, and medical conditions.\nIn this article, we delve into a Spark NLP-based pipeline to convert handwritten prescriptions into structured, machine-readable text. Leveraging BERT embeddings for contextual understanding and a custom Named Entity Recognition(NER) model, this approach promises to streamline information extraction in medical workflows. From OCR text extraction to entity recognition and model training, each step is tailored to maximize accuracy for complex medical terminology.\nGithub Link\n1. Extract Handwritten text using OCR The first and foremost step is to use an OCR to extract handwritten texts from the doctor\u0026rsquo;s prescriptions. I have used AWS Textract that automatically extracts text, handwriting, layout elements, and data from scanned documents.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def detect_text(local_file, region_name, aws_access_key_id, aws_secret_access_key): # Initialize Textract client textract = boto3.client( \u0026#39;textract\u0026#39;, region_name=region_name, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key ) # Open file and detect text with open(local_file, \u0026#39;rb\u0026#39;) as document: response = textract.detect_document_text(Document={\u0026#39;Bytes\u0026#39;: document.read()}) # Extract text from response text_lines = [ item[\u0026#34;Text\u0026#34;] for item in response.get(\u0026#34;Blocks\u0026#34;, []) if item.get(\u0026#34;BlockType\u0026#34;) == \u0026#34;LINE\u0026#34; ] extracted_text = \u0026#34; \u0026#34;.join(text_lines) logger.info(f\u0026#34;Successfully extracted text from {local_file}\u0026#34;) return extracted_text The data extracted from the OCR is then sent further into the NER pipeline, which we will be creating later in this blog, for extraction of Named Entities.\n2. Initializing the NER Model We create a class InitiateNER to initialize the Spark NLP session, embeddings, and NER tagger.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class InitiateNER: def __init__(self, gpu=True, embedding_model=\u0026#39;bert_base_uncased\u0026#39;, language=\u0026#39;en\u0026#39;): # Initialize Spark session self.spark = sparknlp.start(gpu=gpu) logger.info(\u0026#34;Spark NLP session started.\u0026#34;) # Initialize embeddings and NER tagger self.bert_embeddings = BertEmbeddings.pretrained(embedding_model, language) \\ .setInputCols([\u0026#34;sentence\u0026#34;, \u0026#39;token\u0026#39;]) \\ .setOutputCol(\u0026#34;embeddings\u0026#34;) \\ .setCaseSensitive(False) self.ner_tagger = NerDLApproach() \\ .setInputCols([\u0026#34;sentence\u0026#34;, \u0026#34;token\u0026#34;, \u0026#34;embeddings\u0026#34;]) \\ .setLabelColumn(\u0026#34;label\u0026#34;) \\ .setOutputCol(\u0026#34;ner\u0026#34;) \\ .setMaxEpochs(20) \\ .setLr(0.001) \\ .setPo(0.005) \\ .setBatchSize(32) \\ .setValidationSplit(0.1) \\ .setUseBestModel(True) \\ .setEnableOutputLogs(True) # Initialize other components for prediction self.document_assembler = DocumentAssembler() \\ .setInputCol(\u0026#34;text\u0026#34;) \\ .setOutputCol(\u0026#34;document\u0026#34;) self.sentence_detector = SentenceDetector() \\ .setInputCols([\u0026#39;document\u0026#39;]) \\ .setOutputCol(\u0026#39;sentence\u0026#39;) self.tokenizer = Tokenizer() \\ .setInputCols([\u0026#39;sentence\u0026#39;]) \\ .setOutputCol(\u0026#39;token\u0026#39;) self.converter = NerConverter() \\ .setInputCols([\u0026#34;document\u0026#34;, \u0026#34;token\u0026#34;, \u0026#34;ner\u0026#34;]) \\ .setOutputCol(\u0026#34;ner_span\u0026#34;) The NerDLModel is an encoder-decoder neural network (we will talk about it later) which needs the input in the form of embeddings[1]. This can be achieved via a WordEmbeddings model. We are using BertEmbeddings in this case.\nWordEmbeddings model Word embeddings[2] capture semantic relationships between words, allowing models to understand and represent words in a continuous vector space where similar words are close to each other. This semantic representation enables more nuanced understanding of language.\nWhy I chose BERT Embeddings over others? BERT embeddings [3, 4, 5, 6] offer several advantages over traditional embeddings for Named Entity Recognition (NER):\nContextualized Embeddings: BERT creates different embeddings for the same word based on its context, unlike Word2Vec or GloVe, which give each word a single fixed representation. This is crucial in NER, where the meaning of a word can change with context.\nBidirectional Attention: BERT reads sentences in both directions (left-to-right and right-to-left), capturing a full understanding of each word\u0026rsquo;s context. Traditional embeddings lack this bidirectional context, missing nuances especially useful in medical text.\nPre-Trained on Large Data: BERT is pre-trained on a large corpus, which enhances its versatility for fine-tuning in domain-specific applications like medical NER, where context-rich understanding is needed.\nHandling Rare or OOV Words: BERT uses subword tokenization, which allows it to break down unknown words and still retain meaning, a huge advantage for handling medical terminology.\nProven Performance: BERT consistently outperforms older embeddings on NER tasks due to its deep contextual understanding and flexibility in handling diverse language structures.\nNerDLApproach The NerDLApproach[1] in Spark NLP is a neural network model designed for Named Entity Recognition (NER) tasks. It uses a combination of Character-level Convolutional Neural Networks (Char CNNs), Bidirectional Long Short-Term Memory networks (BiLSTMs) (Char CNN - BiLSTM[7]), and followed by a Conditional Random Fields (CRFs). Let\u0026rsquo;s break down why this architecture is used and how it processes normal text input.\nWhy Char CNNs - BiLSTM - CRF? Character-level CNNs (Char CNNs):\nPurpose: Capture morphological features of words, such as prefixes, suffixes, and other subword patterns.\nInput: Character embeddings of each word.\nOutput: Character-level features that are useful for understanding the structure of words, especially for handling out-of-vocabulary words or misspellings.\nBidirectional LSTMs (BiLSTMs):\nPurpose: Capture contextual information from both past (left) and future (right) contexts in the text.\nInput: Word embeddings (e.g., BERT embeddings) and character-level features from Char CNNs.\nOutput: Contextualized word representations that consider the entire sentence.\nConditional Random Fields (CRFs):\nPurpose: Model the dependencies between output labels (e.g., the sequence of named entity tags) to ensure valid sequences. Input: Contextualized word representations from BiLSTMs. Output: The most likely sequence of named entity tags for the input text. Example Let\u0026rsquo;s consider an example to illustrate this process:\nInput Text: \u0026quot;John Doe works at Store Ninja.\u0026quot;\nTokenization:\nTokens: [\u0026quot;John\u0026quot;, \u0026quot;Doe\u0026quot;, \u0026quot;works\u0026quot;, \u0026quot;at\u0026quot;, \u0026quot;Store\u0026quot;, \u0026quot;Ninja\u0026quot;, \u0026quot;.\u0026quot;] Character Embeddings:\nCharacters for \u0026ldquo;John\u0026rdquo;: [\u0026quot;J\u0026quot;, \u0026quot;o\u0026quot;, \u0026quot;h\u0026quot;, \u0026quot;n\u0026quot;] Character embeddings are generated for each character. Char CNNs:\nChar CNNs process the character embeddings to capture morphological features. Word Embeddings:\nWord embeddings (e.g., BERT embeddings) are generated for each token. BiLSTMs:\nBiLSTMs process the word embeddings and character-level features to capture contextual information. For example, the representation for \u0026ldquo;John\u0026rdquo; will consider the context provided by \u0026ldquo;Doe works at Store Ninja.\u0026rdquo; CRFs:\nCRFs predict the sequence of named entity tags. Output: [\u0026quot;B-PER\u0026quot;, \u0026quot;I-PER\u0026quot;, \u0026quot;O\u0026quot;, \u0026quot;O\u0026quot;, \u0026quot;B-ORG\u0026quot;, \u0026quot;O\u0026quot;] I will be explaining all the instance variables as they come in use in other functions. 3. Training the Model The train_model method reads the CoNLL dataset, applies BERT embeddings, and trains the NER model.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def train_model(self, training_file, save_path): try: training_data = CoNLL().readDataset(self.spark, training_file) training_data = self.bert_embeddings.transform(training_data).drop(\u0026#34;text\u0026#34;, \u0026#34;document\u0026#34;, \u0026#34;pos\u0026#34;) logger.info(\u0026#34;Training model...\u0026#34;) self.ner_model = self.ner_tagger.fit(training_data) logger.info(\u0026#34;Model trained successfully.\u0026#34;) self.ner_model.write().overwrite().save(save_path) logger.info(f\u0026#34;Model saved at: {save_path}\u0026#34;) except Exception as e: logger.error(f\u0026#34;Error during training: {e}\u0026#34;) train_model method: Trains the NER model. CoNLL().readDataset: Reads training data in CoNLL format. self.bert_embeddings.transform: Applies BERT embeddings to the training data. self.ner_tagger.fit: Trains the NER model. self.ner_model.write().overwrite().save: Saves the trained model. What is CoNLL format? The CoNLL (Conference on Natural Language Learning) format is a structured text format commonly used to label tokens in natural language processing tasks, especially named entity recognition (NER). In a CoNLL-formatted dataset, each line represents a token (such as a word or punctuation), with additional columns typically containing labels or annotations about that token. Sentences are separated by blank lines.\nCoNLL Format Structure A typical CoNLL dataset has multiple columns for each token. The columns can vary based on the dataset\u0026rsquo;s intended task, but for NER, they usually look like this:\nToken POS Chunk Entity John NNP B-NP B-PER lives VBZ B-VP O in IN B-PP O New NNP B-NP B-LOC York NNP -X- I-LOC . . O O In this example:\nEach line corresponds to a word, and there may be additional columns with part-of-speech (POS) or chunk tags. The \u0026ldquo;Entity\u0026rdquo; column marks NER tags (e.g., B-PER for \u0026ldquo;Beginning of a PERSON entity\u0026rdquo; and O for \u0026ldquo;Outside of any entity\u0026rdquo;). Blank lines separate sentences. -X- represents that is often used as a placeholder or dummy value for certain columns, especially when the information in that column is not relevant for a particular token. The dataset I am using looks something like this -\nToken POS Chunk Entity negative -X- -X- O for -X- -X- O chest -X- -X- B-Medical_condition pain, -X- -X- I-Medical_condition pressure, -X- -X- B-Medical_condition dyspnea, -X- -X- B-Medical_condition edema -X- -X- B-Medical_condition or -X- -X- O cold -X- -X- B-Medical_condition symptoms. -X- -X- O As we are only interested in Entity mapping, the POS and Chunk values are irrelevant to us, hence we have put a -X- or Empty Placeholder there.\nWhy CoNLL Format Is Used in NER The CoNLL format is widely used in NER for several reasons:\nStructured yet simple: It provides a structured, line-by-line format that’s easy to parse and interpret, both by humans and machines. Consistent token-label pairing: Each token is directly paired with its label, simplifying the process of supervised learning where the model learns to associate tokens with specific tags. Sentence separation: Blank lines make sentence boundaries clear, which is crucial for tasks like NER, where context within sentences matters. Widely supported by NLP tools: Many NLP frameworks (like spaCy, NLTK, and transformers libraries) support CoNLL format, so training data in this format is easily integrated into NER pipelines. Where to get Clinical NER Datasets? Such datasets can be found at various sources -\nCoNLL-2003[8] 2010 i2b2/VA[9] 2014 i2b2 De-identification Challenge 2018 n2c2 Medication Extraction Challenge 4. Loading the Model 1 2 3 4 5 6 7 8 9 def load_model(self, model_path): try: self.loaded_ner_model = NerDLModel.load(model_path) \\ .setInputCols([\u0026#34;sentence\u0026#34;, \u0026#34;token\u0026#34;, \u0026#34;embeddings\u0026#34;]) \\ .setOutputCol(\u0026#34;ner\u0026#34;) logger.info(\u0026#34;Model loaded successfully.\u0026#34;) except Exception as e: logger.error(f\u0026#34;Error loading model: {e}\u0026#34;) load_model method: Loads a pre-trained NER model. NerDLModel.load: Loads the model from the specified path. 5. Making Predictions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def predict(self, text): try: # Create or reuse prediction pipeline if not hasattr(self, \u0026#39;prediction_pipeline\u0026#39;): self.prediction_pipeline = Pipeline( stages=[ self.document_assembler, self.sentence_detector, self.tokenizer, self.bert_embeddings, self.loaded_ner_model, self.converter ] ) logger.info(\u0026#34;Prediction pipeline created.\u0026#34;) sample_data = self.spark.createDataFrame([[text]]).toDF(\u0026#34;text\u0026#34;) prediction_model = self.prediction_pipeline.fit(self.spark.createDataFrame([[\u0026#39;\u0026#39;]]).toDF(\u0026#34;text\u0026#34;)) preds = prediction_model.transform(sample_data) pipeline_result = preds.collect()[0] return self.format_output(pipeline_result) except Exception as e: logger.error(f\u0026#34;Error during prediction: {e}\u0026#34;) return {\u0026#34;error\u0026#34;: str(e)} Pipeline Stages: The pipeline consists of several stages:\ndocument_assembler: Converts raw text into a structured document format (read about it below). sentence_detector: Detects sentences within the document. tokenizer: Tokenizes sentences into individual words. bert_embeddings: Applies BERT embeddings to the tokens. loaded_ner_model: Uses the pre-trained NER model to identify named entities. converter: Converts the NER results into a more readable format. Pipeline Example If the input text is \u0026quot;John Doe works at OpenAI.\u0026quot;, the pipeline stages will process it as follows:\nDocumentAssembler: Converts the text into a document. SentenceDetector: Identifies \u0026quot;John Doe works at OpenAI.\u0026quot; as a single sentence. Tokenizer: Tokenizes the sentence into [\u0026quot;John\u0026quot;, \u0026quot;Doe\u0026quot;, \u0026quot;works\u0026quot;, \u0026quot;at\u0026quot;, \u0026quot;OpenAI\u0026quot;, \u0026quot;.\u0026quot;]. BertEmbeddings: Generates embeddings for each token. Loaded NER Model: Identifies \u0026quot;John Doe\u0026quot; as a person and \u0026quot;OpenAI\u0026quot; as an organization. Converter: Converts the NER results into a readable format. self.spark.createDataFrame: Creates a Spark DataFrame from the input text.\nself.prediction_pipeline.fit: Fits the pipeline (required for some Spark operations).\nprediction_model.transform: Transforms the input text using the pipeline.\npreds.collect: Collects the prediction results.\nself.format_output: Formats the prediction results.\nWhat\u0026rsquo;s a Document? In the context of Natural Language Processing (NLP) and Spark NLP, a \u0026ldquo;document\u0026rdquo; refers to a structured representation of text data. This structured representation is used as an intermediate format that allows various NLP components to process the text more effectively. Let\u0026rsquo;s break down what this means in more detail:\nDocument in Spark NLP A \u0026ldquo;document\u0026rdquo; in Spark NLP is essentially a DataFrame column that contains metadata about the text, such as its content, the start and end positions of the text, and other relevant information. This structured format is crucial for the subsequent NLP tasks, such as sentence detection, tokenization, and named entity recognition.\nDocumentAssembler The DocumentAssembler[10] is a component in Spark NLP that converts raw text into this structured \u0026ldquo;document\u0026rdquo; format. Here\u0026rsquo;s how it works:\nInput Column: The DocumentAssembler takes a column of raw text as input. Output Column: It produces a column of \u0026ldquo;document\u0026rdquo; type, which contains the structured representation of the text. Example Let\u0026rsquo;s look at an example to understand this better:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from sparknlp.base import DocumentAssembler from pyspark.sql import SparkSession # Initialize Spark session spark = SparkSession.builder \\ .appName(\u0026#34;DocumentAssemblerExample\u0026#34;) \\ .getOrCreate() # Sample DataFrame with raw text data = spark.createDataFrame([[\u0026#34;This is a sample text.\u0026#34;]]).toDF(\u0026#34;text\u0026#34;) # Initialize DocumentAssembler document_assembler = DocumentAssembler() \\ .setInputCol(\u0026#34;text\u0026#34;) \\ .setOutputCol(\u0026#34;document\u0026#34;) # Transform the DataFrame document_df = document_assembler.transform(data) # Show the result document_df.select(\u0026#34;document\u0026#34;).show(truncate=False) Output The output will be a DataFrame with a \u0026ldquo;document\u0026rdquo; column that contains the structured representation of the text:\n+-------------------------------------------------------------+ |document | +-------------------------------------------------------------+ |[[document, 0, 20, This is a sample text., [sentence -\u0026gt; 0], []]]| +-------------------------------------------------------------+ Explanation document: The column name for the structured text. 0, 20: The start and end positions of the text. This is a sample text.: The actual text content. [sentence -\u0026gt; 0]: Metadata indicating that this is the first sentence. Why is this Important? The structured \u0026ldquo;document\u0026rdquo; format is essential for the following reasons:\nConsistency: It provides a consistent way to represent text data, making it easier to process. Metadata: It includes metadata that can be useful for various NLP tasks. Pipeline Integration: It allows different components in the NLP pipeline to work together seamlessly. Summary 1. Training the NER Model 2. OCR and NER Pipeline for Prescription Processing OCR (Optical Character Recognition)\nConverts handwritten doctor prescriptions into machine-readable text using OCR techniques. NER (Named Entity Recognition)\nInput: Text from the OCR step containing unstructured data such as medicine names and dosage instructions. BERT Embedding: Converts the input text into context-aware embeddings. CHAR CNN-BiLSTM: Character-level CNN captures morphological features of words. BiLSTM captures bidirectional context of the text sequence. CRF (Conditional Random Field): Ensures valid label sequences for structured output like medicine names, dosages, and eating schedules. Output: Structured data with medicine names, dosages, and schedules extracted from the text.\nActual Output Handwritten Doctor\u0026rsquo;s Prescription Output Conclusion In this blog, we explored a comprehensive pipeline for extracting information from handwritten medical prescriptions using AWS Textract and Spark NLP. Starting with the OCR capabilities of AWS Textract, we efficiently transformed handwritten text into machine-readable format, setting the stage for the subsequent NER analysis.\nAs we concluded with predictions, the seamless interaction of components within the prediction pipeline demonstrated the power of Spark NLP for real-time NER tasks. This approach not only streamlines the processing of medical prescriptions but also holds potential for further applications in healthcare, where accurate data extraction is paramount.\nBy combining cutting-edge technology with thoughtful design, we can significantly enhance the efficiency of healthcare processes, ultimately leading to improved patient outcomes and better management of medical information. As we continue to innovate in this space, the opportunities for developing advanced applications in the realm of healthcare data processing are endless.\nReferences [1] “Spark NLP 5.5.1 ScalaDoc - com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach,” Sparknlp.org, 2024. https://sparknlp.org/api/com/johnsnowlabs/nlp/annotators/ner/dl/NerDLApproach.\n[2] J. Barnard, “What are Word Embeddings? | IBM,” www.ibm.com, Jan. 23, 2024. https://www.ibm.com/topics/word-embeddings\n[3] Contextualized Embeddings and Bidirectional Attention: Devlin, J., Chang, M.-W., Lee, K., \u0026amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of NAACL-HLT, 4171–4186. https://arxiv.org/abs/1810.04805.\n[4] Handling Out-of-Vocabulary Words and Subword Tokenization: Sennrich, R., Haddow, B., \u0026amp; Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1715–1725. https://arxiv.org/abs/1508.07909.\n[5] Fine-Tuning and Performance on NER: Akbik, A., Blythe, D., \u0026amp; Vollgraf, R. (2018). Contextual String Embeddings for Sequence Labeling. Proceedings of the 27th International Conference on Computational Linguistics, 1638–1649. https://www.aclweb.org/anthology/C18-1139/.\n[6] BERT’s Performance in Domain-Specific Applications: Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., \u0026amp; Kang, J. (2020). BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4), 1234-1240. https://academic.oup.com/bioinformatics/article/36/4/1234/5566506.\n[7] J. P. C. Chiu and E. Nichols, “Named Entity Recognition with Bidirectional LSTM-CNNs,” Transactions of the Association for Computational Linguistics, vol. 4, pp. 357–370, Dec. 2016, doi: https://doi.org/10.1162/tacl_a_00104\n[8] “Papers with Code - CoNLL-2003 Dataset,” paperswithcode.com. https://paperswithcode.com/dataset/conll-2003\n[9] “Papers with Code - 2010 i2b2/VA Dataset,” Paperswithcode.com, 2022. https://paperswithcode.com/dataset/2010-i2b2-va.\n[10] “Spark NLP 5.5.1 ScalaDoc - com.johnsnowlabs.nlp.DocumentAssembler,” Sparknlp.org, 2024. https://sparknlp.org/api/com/johnsnowlabs/nlp/DocumentAssembler.\n","permalink":"https://aryaman.space/blog/medical-text-extraction/","summary":"Introduction Medical prescriptions, often scribbled in hurried handwriting, pose a significant challenge when attempting to extract valuable information.\nAutomating this process requires a robust combination of Optical Character Recognition (OCR) and Natural Language Processing (NLP) tools to accurately identify entities like medication names, dosages, and medical conditions.\nIn this article, we delve into a Spark NLP-based pipeline to convert handwritten prescriptions into structured, machine-readable text. Leveraging BERT embeddings for contextual understanding and a custom Named Entity Recognition(NER) model, this approach promises to streamline information extraction in medical workflows.","title":"From Scribbles to Structured Data: Processing Handwritten Prescriptions with Spark NLP"},{"content":"Making a MongoDB cluster \u0026ldquo;Production Ready\u0026rdquo; by performing database Replication and Sharding (horizontal fragmentation)\nWhat is Replication? Replication is the method of duplication of data across multiple servers. For example, we have an application and it reads and writes data to a database and says this server A has a name and balance which will be copied/replicate to two other servers in two different locations.\nBy doing this, will get redundancy and increases data availability with multiple copies of data on different database servers. So, it will increase the performance of reading scaling. The set of servers that maintain the same copy of data is known as replica servers or MongoDB instances.\nWhy Replication? High Availability of data disasters recovery\nNo downtime for maintenance ( like backups index rebuilds and compaction)\nRead Scaling (Extra copies to read from)\nWhat is Sharding? Sharding is a method for allocating data across multiple machines. MongoDB used sharding to help deployment with very big data sets and large throughput the operation. By sharding, you combine more devices to carry data extension and the needs of read and write operations.\nA sharded cluster consists of 3 things -\nShards - A replica set that contains a sunset of the cluster\u0026rsquo;s data\nMongos - For a sharded cluster, mongos provides an interface between client applications and sharded cluster\nConfig Servers - They are the authoritative source of sharding metadata. The metadata contains the list of sharded collections, routing info etc.\nWhy Sharding? Database systems having big data sets or high throughput requests can doubt the ability of a single server.\nFor example, High query flows can drain the CPU limit of the server.\nThe working set sizes are larger than the system\u0026rsquo;s RAM to stress the I/O capacity of the disk drive.\nData Replication in action Insert data into primary repl\nGoing into a secondary repl and querying peaks collection in peaksDB gives us the replicated result -\nWE CANNOT WRITE INTO A REPLICATIONED/SECONDARY DATABASE, WRITES ARE ONLY ALLOWED IN THE PRIMARY REPL.\nSteps to create the above sharded cluster Go to \\mongo\\data\\ folder\nMake cfg0, cfg1, cfg2 (representing 3 replicas of the config server)\nMake a0,a1,a2, b0,b1,b2, c0,c1,c2 (represents the replicas of shard a, b, c respectively)\nRun the 3 config servers in different terminals -\n1 mongod --configsvr --dbpath cfg0 --port 26050 --logpath log.cfg0 --replSet cfg 1 mongod --configsvr --dbpath cfg1 --port 26051 --logpath log.cfg1 --replSet cfg 1 mongod --configsvr --dbpath cfg2 --port 26052 --logpath log.cfg2 --replSet cfg Here,\u0026ndash;configsvr represents that we are creating a config server, –dbpath represents the folder of that config replica, –port represents the port number on which I want the server to run, –logpath represents the log file of the particular server, and –replSet represents the name of the Replication Set.\nAttaching mongo shell to the config server running on port 26050 (as we want to make this server as the Primary Replica of the replSet)\n1 mongosh --port 26050 Initiate Replication Set for the config server, and add the other 2 servers to the set -\n1 2 3 rs.initiate() rs.add(\u0026#34;localhost:26051\u0026#34;) #Adding the other config server to the replSet rs.add(\u0026#34;localhost:26052\u0026#34;) Doing rs.status() we can see the members of the replica set -\nNow launch the servers for the shard \u0026lsquo;a\u0026rsquo; having replicas a0, a1, a2 in it\u0026rsquo;s replSet (each server should be launched in different terminals) -\n1 mongod --shardsvr --replSet a --dbpath a0 --port 26000 --logpath log.a0 1 mongod --shardsvr --replSet a --dbpath a1 --port 26001 --logpath log.a1 1 mongod --shardsvr --replSet a --dbpath a2 --port 26002 --logpath log.a2 Do similar to create shards \u0026lsquo;b\u0026rsquo; and \u0026lsquo;c\u0026rsquo; -\n1 2 3 mongod --shardsvr --replSet b --dbpath b0 --port 26100 --logpath log.b0 mongod --shardsvr --replSet b --dbpath b1 --port 26101 --logpath log.b1 mongod --shardsvr --replSet b --dbpath b2 --port 26102 --logpath log.b2 1 2 3 mongod --shardsvr --replSet c --dbpath c0 --port 26200 --logpath log.c0 mongod --shardsvr --replSet c --dbpath c1 --port 26201 --logpath log.c1 mongod --shardsvr --replSet c --dbpath c2 --port 26202 --logpath log.c2 We can see the servers are actively \u0026ldquo;LISTENING\u0026rdquo; on all the ports -\nNow open a new terminal and connect a mongo shell to server running on port 26000, initiate a replSet and add replicas a1 and a2 to it (Inter-connecting the replSet \u0026lsquo;a\u0026rsquo;) -\n1 mongosh --port 26000 1 2 3 rs.initiate() rs.add(\u0026#34;localhost:26001\u0026#34;) rs.add(\u0026#34;localhost:26002\u0026#34;) Do the same for replSet \u0026lsquo;b\u0026rsquo; and \u0026lsquo;c\u0026rsquo; -\n1 2 3 4 mongosh --port 26100 rs.initiate() rs.add(\u0026#34;localhost:26101\u0026#34;) rs.add(\u0026#34;localhost:26102\u0026#34;) 1 2 3 4 mongosh --port 26200 rs.initiate() rs.add(\u0026#34;localhost:26201\u0026#34;) rs.add(\u0026#34;localhost:26202\u0026#34;) The next step is to start the mongos instance (in a new terminal) which would be the interaction point of the client with the sharded environment -\n1 mongos --configdb \u0026#34;cfg/localhost:26050,localhost:26051,localhost:26052\u0026#34; --logpath log.mongos1 --port 26061 Here, –configdb represents the ip address of the 3 config servers we created earlier.\nNow connect a mongo shell to this mongos instance (in a new terminal) and start adding the shards to it -\n1 2 3 4 mongosh --port 26061 sh.addShard(\u0026#34;a/*[*localhost:26000*](http://localhost:26000)*\u0026#34;) sh.addShard(\u0026#34;b/localhost:26100\u0026#34;) sh.addShard(\u0026#34;c/localhost:26200\u0026#34;) In sh.addShard(\u0026ldquo;a/localhost:26000\u0026rdquo;), \u0026lsquo;a\u0026rsquo; represents replSet, and localhost:26000 represents the Primary Replica of \u0026lsquo;a\u0026rsquo; replSet.\nTo see the status of the sharded environment -\n1 sh.status() We can see that the 3 sharded replSets have been added. Also 1 mongos instance is active.\nAlso, we can see that the currently sharded database is only config db, as we have not sharded any other database.\nLets add peaksDB for sharding -\n1 sh.enableSharding(\u0026#34;peaksDB\u0026#34;) Now doing sh.status(), we can see \u0026ldquo;peaksDB\u0026rdquo; in \u0026ldquo;databases\u0026rdquo; field -\nWe can see 1 sample document inside our \u0026lsquo;peaksDB.peaks\u0026rsquo; collection. Lets shard the collection on \u0026rsquo;name\u0026rsquo; field -\nCreate an index of the proposed shard key -\n1 db.peaks.ensureIndex({name: \u0026#34;hashed\u0026#34;}) Shard the collection using that key -\n1 sh.shardCollection(\u0026#34;peaksDB.peaks\u0026#34;, {name:\u0026#34;hashed\u0026#34;}) Now doing sh.status() we can see that for our collection \u0026ldquo;peaksDB.peaks\u0026rdquo; has a shardKey of \u0026rsquo;name\u0026rsquo; and is present on shard \u0026lsquo;c\u0026rsquo; -\nFinally, we can get more details of all the shards present -\\\n1 2 use config db.shards.find() References mongodb.com/docs/manual/replication/\nmongodb.com/docs/manual/sharding/\nmongodb.com/docs/manual/core/sharded-cluster-config-servers/\nyoutube.com/@vemarahub\nengineeringdigest.net\n","permalink":"https://aryaman.space/blog/replication-and-sharding-in-mongo/","summary":"Making a MongoDB cluster \u0026ldquo;Production Ready\u0026rdquo; by performing database Replication and Sharding (horizontal fragmentation)\nWhat is Replication? Replication is the method of duplication of data across multiple servers. For example, we have an application and it reads and writes data to a database and says this server A has a name and balance which will be copied/replicate to two other servers in two different locations.\nBy doing this, will get redundancy and increases data availability with multiple copies of data on different database servers.","title":"Demystifying Replication and Sharding in MongoDB"},{"content":"Python faces challenges in fully exploiting the growing capabilities of modern hardware. As hardware continues to advance with more CPU cores, faster processors, and abundant memory, Python\u0026rsquo;s inherent design and execution model can often fall short in taking full advantage of these resources. Its single-threaded nature and certain architectural choices can result in suboptimal performance in scenarios where parallelism and hardware acceleration are vital. This limitation prompts developers to seek alternative solutions, such as integrating Python with external libraries, languages, or technologies, to overcome these hardware-related constraints.\nNeed for Simultaneous Execution in Python Python is widely praised for being easy to learn, easy to use, and highly versatile, supporting developer productivity. However, Python is also notoriously slow. Programs written in languages such as C++, Node.js, and Go can execute as much as 30-40 times faster than equivalent Python programs.\nTable Details Table 1.1: Courtesy of github project by Kostya M Even though Python is constantly being optimized to work faster, we simply can\u0026rsquo;t rely on the basic single-threaded execution codes as they are too slow. Hence the need for simultaneous execution.\nSimultaneous Execution Possibilities in Python Three big contenders for dealing with multiple tasks in Python:\nAsyncio: In computer programming, asynchrony encompasses events that occur independently of the primary program flow and methods for managing these events. In essence, asynchronization implies that you can proceed with executing other sections of code without the need to wait for a particular task to finish.\nCooperative pausing/waiting Good for I/O bound processes Threading:\nPython runs on a single thread on a single CPU. GIL is a global lock around the entire Python interpreter. In order to advance the interpreter state and run the Python code, a thread must require the GIL. Hence, it is possible to have multiple Python threads in the same process, but only one of them can be executing the Python code. While this happens, the rest must wait to receive the GIL. Non-cooperative pausing/interrupting Good for I/O bound Good for long-running operations without blocking (e.g., GUI applications) Multiprocessing:\nCreate different processes which have their own GIL. Better where all processes are completely independent of each other. Threading Let\u0026rsquo;s take a for loop as an example, this loop is a CPU-intensive task.\n1 2 3 4 5 6 7 8 9 10 11 # Code 4.1: CPU Intensive task example DO = 100000000 ans = 1 def foo(n): global ans for i in range(n): ans += 2 foo(DO) print(ans) As this was a simple single threaded task, it took around 5 secs and was 99% CPU intensive.\nOkay, so if I have 2 loops like this running on a single thread, it would take around 10s. This is very bad.\n1 2 3 4 5 6 7 8 9 10 11 12 # Code 4.2: Extension of Code 4.1, 2 CPU Intensive tasks DO = 100000000 ans = 1 def foo(n): global ans for i in range(n): ans += 2 foo(DO) foo(DO) print(ans) No issues, as these are CPU intensive tasks, we can run both the tasks on different threads. Hence, simultaneous execution would help us run 2 threads simultaneously and concurrently and the time would be reduced by half.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Code 4.3: CPU Intensive tasks executed in different threads from threading import Thread DO = 100000000 ans = 1 def foo(n): global ans for i in range(n): ans += 2 t1 = Thread(target=foo, args=(DO//2,)) t2 = Thread(target=foo, args=(DO//2,)) t1.start() t2.start() t1.join() t2.join() print(ans) If this was true multithreading, the time taken for a for loop of half the range of the previous example should be half of the previous time taken, i.e. around 2.5 sec.\nBut we can see it again took roughly the same time. Why is this happening? To understand this we need to look into the core concept of Cpython - GIL.\nGIL?! What is GIL? Before discussing GIL, it is important to note that we are discussing this in terms of CPython.\nPython - definition of programming language we know, PEP - python enhancement proposals\nCpython - physical implementation of the ideas in \u0026ldquo;Python\u0026rdquo; in C language\nGenerally when we talk about python, we refer to the Cpython implementation of the language only.\nMoving forward -\nFigure DetailsCode 4.1.1: foo function I created a function foo() where I allocated a = 50.\n50 will be stored in memory and the variable a would be pointing to it. So the reference count of the memory where 50 is stored would be 1. But now, when the function is finished, the memory location has to be cleared/dereferenced. Hence, the reference count would be decreased by 1.\nReference counting in CPython - At a very basic level, a Python object\u0026rsquo;s reference count is incremented whenever the object is referenced, and it\u0026rsquo;s decremented when an object is dereferenced*.*\nThe memory deallocation is done in different ways in different languages but in python when the reference count of a memory location becomes 0, python knows that that memory location can be used for storing something else.\nExample - Garbage collectors is used by java\nFigure DetailsCode 4.1.2a: Reference Counting Then, why do we get a reference count of \u0026lsquo;a\u0026rsquo; as 3 here?\nFigure DetailsFIgure 4.1.1: Reference Counting explanation This is the reason.\nFigure DetailsCode 4.1.2b: Reference Counting The main issue comes, when there is concurrency involved in python threads -\nFigure DetailsFigure 4.1.2: Issue in concurrency in python due to reference counting Imagine 2 threads are running simultaneously, in CPython. Reference Count of memory location pointed by variable \u0026lsquo;a\u0026rsquo; is 3. Thread 1 and 2 ask what is the reference count of \u0026lsquo;a\u0026rsquo; and both get 3. Now I do \u0026lsquo;b=a\u0026rsquo; in thread 1 so the reference count increases to 4. At the same time I do \u0026lsquo;c=a\u0026rsquo; in thread 2 and it also updates the reference count to 4.\nBut shouldn\u0026rsquo;t the reference count be 5? Race conditions occur in such cases. Hence, the requirement for concurrency management or GIL (Global Interpreter Lock) is required\nFigure DetailsFigure 4.1.3: The GLOBAL INTERPRETER LOCK helps maintain reference count In a real scenario if I have 4 threads running concurrently, thread 1 acquires the GIL, does some computation and waits for some I/O. In that time, the GIL is transferred to thread 2 which again does some task and starts waiting for something, for example a mouse click in the GUI. Again the GIL is transferred to a different thread. That\u0026rsquo;s why multithreading works in cpython.\nBut if I have 2 different threads and both want to work with the CPU, then GIL is not a good option.\nPython 1.0 (1994): Python was initially designed without the GIL. In its early versions, Python used a simple reference counting mechanism for memory management. However, this approach had limitations, especially when dealing with circular references, which could lead to memory leaks.\nPython 1.5 (1999): With Python 1.5, Guido van Rossum, the creator of Python, introduced the Global Interpreter Lock (GIL) as a solution to the multi-threading problems.\nReferring back to the example code 4.3-\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Code 4.3: CPU Intensive tasks executed in different threads from threading import Thread DO = 100000000 ans = 1 def foo(n): global ans for i in range(n): ans += 2 t1 = Thread(target=foo, args=(DO//2,)) t2 = Thread(target=foo, args=(DO//2,)) t1.start() t2.start() t1.join() t2.join() print(ans) If this was true multithreading, the time taken for a for loop of half the range of the previous example should be half of the previous time taken, i.e. around 2.5 sec.\nBut this didn\u0026rsquo;t happen. This means that as both are around 99-100% CPU bound, both require GIL to execute. Hence, when 1 thread acquires the GIL, the other thread waits to receive the GIL back\u0026hellip; hence the time required was roughly the same. Practically they never ran truly parallelly.\nSo, how do we run threads truly concurrently in Cpython? Many times we don\u0026rsquo;t even realize the bottleneck capacity of Cpython as majority of times we are not working with purely python code, for example working with numpy array, numba etc. we can get away with using multithreading/without acquiring GIL as once GIL is acquired, python calls a C program which runs in the background, and hence in that time the GIL can be given to other thread.\nGIL History and Removal Efforts GIL was first introduced in 1999, and had several positive and negative ramifications:\nPositives:\nSimple Easy to get right No deadlocks as there is only one lock Single threading is fast! Good for I/O bound threads Negatives:\nBad for CPU bound tasks, as execution will always be on a single core. This was fine in 1992 as we would rarely see multi-core CPUs back then. But the world has changed! Unfortunately until and unless GIL is present this limitation would always be there, hence GIL removal has been a huge topic of discussion in the Cpython implementation.\nFigure DetailsFigure 4.2.2: https://peps.python.org/pep-0703/ In PEP - Python Enhancement Proposals - a proposal was made, to make the GIL optional, proposing to adding a build configuration and to let it run Python code without the global interpreter lock and with the necessary changes needed to make the interpreter thread-safe.\nOver the years, various efforts have been made to overcome the limitations posed by the GIL:\nFree-Threading (1996): An early attempt to address the GIL was called \u0026ldquo;free-threading,\u0026rdquo; but it didn\u0026rsquo;t fully remove the GIL and had its own set of complexities.\nGilectomy (2015): This initiative aimed to remove the GIL from CPython entirely. However, it turned out to be a challenging task due to the intricacies of Python\u0026rsquo;s memory management and extensive use of C libraries.\nRecent Developments: \u0026ldquo;nogil\u0026rdquo; have been proposed, which explore ways to make Python more thread-friendly and possibly eliminate the GIL. These proposals aim to allow more parallelism in certain scenarios.\nIn the following sections, we would compare the performance improvements of these efforts done by other people and see how much improvements they actually give and at what costs.\nThere are three important aspects of Python to consider if we want fast, free-threading in CPython:\nSingle-threaded performance\nParallelism\nMutability (the ability to change the state or content of objects in a programming language, meaning that mutable objects can be modified after they are created)\nThe last of these, mutability, is key.\nAlmost all of the work done on speeding up dynamic languages stems from original work done on the self programming language, but with many refinements. Most of this work has been on Javascript, which is single-threaded and less mutable than Python, but PyPy has also made some significant contributions. PyPy has a GIL.\nSelf is an object-oriented programming language based on the concept of prototypes,\nThere is also work on Ruby. Ruby is even more mutable than Python, but also has a GIL. Work on Ruby mirrors that on Javascript.\nThe JVM supports free threading, but Java objects are almost immutable compared to Python objects, and the performance of Python, Javascript and Ruby implementations on the JVM has been disappointing, historically.\nPerforming the optimizations necessary to make Python fast in a free-threading environment will need some original research. That makes it more costly and a lot more risky.\nThere are three options available to the Steering Council (with regard to PEP 703:\nChoose single-threading performance: We ultimately expect a 5x speedup over 3.10\nChoose free-threading: NoGIL appears to scale well, but expect worse single threaded performance.\nChoose both.\nThe pros and cons of the three options Option 1:\nPros: We know how to do this. We have a plan. It will happen. Sub-interpreters allow some limited form of parallelism.\nCons: It keeps the GIL, preventing free-threading.\nOption 2:\nPros: Free threading; much better performance for those who can use multiple threads effectively.\nCons: Some unknowns. Worse performance for most people, as single threaded performance will be worse. We don\u0026rsquo;t know how much worse.\nOption 3:\nPros: Gives us the world class runtime that the world\u0026rsquo;s number one programming language should have.\nCons: More unknowns and a large cost.\nFree-Threading Patch - link Released in 1996, the Free Threading patch was built for Python 1.4 by Greg Stein. Unfortunately, Python 1.4 is very old and cannot be built on modern systems of today\u0026rsquo;s times. Hence, we would be looking into benchmarkings done by Dave Beazley in 2011.\nFirst, he wrote a simple spin-loop and see what happened:\nUsing the original version of Python-1.4 (with the GIL), this code ran in about 1.9 seconds. Using the patched GIL-less version, it ran in about 12.7 seconds. That\u0026rsquo;s about 6.7 times slower. Yow!\nJust to further confirm his findings, he ran the included Tools/scripts/pystone.py benchmark (modified to run slightly longer in order to get more accurate timings).\nFirst, with the GIL:\nNow, without the GIL:\nHere, the GIL-less Python is only about 4 times slower.\nTo test threads,he wrote a small sample that subdivided the work across two worker threads is an embarrassingly parallel manner (note: this code is a little wonky due to the fact that Python-1.4 doesn\u0026rsquo;t implement thread joining\u0026ndash;meaning that we have to do it ourselves with the included binary-semaphore lock).\nIf we run this code with the GIL, the execution time is about 2.5 seconds or approximately 1.3 times slower than the single-threaded version (1.9 seconds). Using the GIL-less Python, the execution time is 18.5 seconds or approximately 1.45 times slower than the single-threaded version (12.7 seconds). Just to emphasize, the GIL-less Python running with two-threads is running more than 7 times slower than the version with a GIL. Ah, but what about preemption you ask? If we return to the example above in the section about reentrancy, we will find that removing the GIL, does indeed, allow free threading and long-running calculations to be preempted. Success! Needless to say, there might be a few reasons why the patch quietly disappeared.\nGilectomy by Larry Hastings - link The earlier effort of removing GIL was more of a failure, as it worsened single thread performance as well as the multi threaded program ran slower compared running the same on single thread.\nHence, 3 political considerations Larry Hastings tried to keep in mind while building Gilectomy were -\nDon\u0026rsquo;t hurt single threaded performance\nDon\u0026rsquo;t break c extensions\nDon\u0026rsquo;t make it too complicated\nSo, the things he did were -\nKeep reference counting - It is a core functionality of Cpython and changing it would mean writing all the C APIs.\nUsed atomic incr/decr which comes free with Intel Processor. But is 30% slower off the top Global and static variables\nShared singletons - all variables (in thread implementation) are public everytime Atomicity - This is where the one big lock (GIL) would be replaced by smaller locks\nHence, a new lock api was introduce Here is a benchmarking code given by Larry Hastings himself, as the Gilectomy implementation has a lot of things like \u0026ldquo;str\u0026rdquo; etc. which don\u0026rsquo;t work as intended.\nTo benchmark, this code was run using both, Cpython (GIL) and Gilectomy (noGIL), on a 8 core, 2.80GHz system.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Code 4.4.1: Benchmarking Code of Gilectomy - # https://github.com/larryhastings/gilectomy/blob/gilectomy/x.py import threading import sys def fib(n): if n \u0026lt; 2: return 1 return fib(n-1) + fib(n-2) def test(): print(fib(30)) threads = 1 #changing number of threads here if len(sys.argv) \u0026gt; 1: threads = int(sys.argv[1]) for i in range(threads - 1): threading.Thread(target=test).start() if threads \u0026gt; 0: test() This code performs a CPU intensive task of calculating fibonacci of a large number (30 in this case) and running that CPU heavy task on multiple threads.\nThis code was ran on both Cpython-3.11 and Gilectomy-3.6.0a1\nThe reason for providing a benchmarking code by Larry was that many **features in Cpython did not work as it is in this Gilectomy\u0026rsquo;s implementation of python. Something as simple as \u0026lsquo;str\u0026rsquo; (string) had a very ambiguous behavior hence, he had to provide a code which could run on both Cpython and Gilectomy.\nFigure DetailsOutput 4.4.1 - Output for Benchmarking Code 4.4.1 Graph DetailsGraph 4.4.1 - Cpyton(GIL) vs Gilectomy(no GIL) Wall Time taken by code to complete execution - comparison on multithreaded code This graph represents the Wall Time difference in time taken by both the versions of Python. We can see the Gilectomy implementation is 2x times slower, which is not bad.\nGraph DetailsGraph 4.4.2 - Cpyton(GIL) vs Gilectomy(no GIL) CPU Time taken by code to complete execution - comparison on multithreaded code But here we can see that in terms of CPU time, Gilectomy is nearly 2x slower at 1 thread, but jumps to 10x slower at 2 threads and from there it keeps going up and up. At 17 cores, it is nearly 19-20 times slower.\nIn the context of this code, more CPU time is typically considered \u0026ldquo;bad\u0026rdquo; or inefficient. If you\u0026rsquo;re running multiple threads and each thread consumes a lot of CPU time, it can lead to high CPU utilization, potentially causing the system to become unresponsive for other tasks.\nWhat happened to Gilectomy later?\nThe last update in Pycon 2019 was that after significant work Larry Hastings was still able to get the performance of his non-GIL version to match that of to Python with the GIL, but with a significant caveat; the default Python version ran only on a single core (as expected) - the non-GIL version needed to run on 7 cores to keep up. Larry Hastings later admitted that work has stalled, and he needs a new approach since the general idea of trying to maintain the general reference counting system, but protect the reference counts without a Global lock is not tenable.\nnogil by Sam Gross - link nogil is a proof-of-concept implementation of CPython that supports multithreading without the global interpreter lock (GIL). The purpose of this project was to show -\nThat it is feasible to remove the GIL from Python.\nThat the tradeoffs are manageable and the effort to remove the GIL is worthwhile.\nThat the main technical ideas of this project (reference counting, allocator changes, and thread-safety scheme) should serve as a basis of such an effort.\nFigure DetailsFigure 4.5.1: nogil speed-up on the pyperformance benchmark suite compared to Python 3.9.0a3. Benchmarking across existing Cpython modules in nogil Python implementation to check speedup/speeddown Let\u0026rsquo;s test on the same code, but calculating fib of 40 now -\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Code 4.5.1: Benchmarking code of CPU intensive task of calculating # fibonacci of a large number, code ran on both Cpython-3.11 # and nogil-3.9.10-1 import threading import sys def fib(n): if n \u0026lt; 2: return 1 return fib(n-1) + fib(n-2) def test(): print(fib(40)) # test function that calculates fibonacci number threads = 7 # number of threads if len(sys.argv) \u0026gt; 1: threads = int(sys.argv[1]) for i in range(threads - 1): threading.Thread(target=test).start() if threads \u0026gt; 0: test() Figure DetailsOutput 4.5.1 - Output for Benchmarking Code 4.5.1 As we can see here, running code on nogil python implementation helped us to utilize more than 1 core at the same time without any restrictions, and all the 7 threads created are utilizing 100% of the respective CPUs.\nIn the same multithreaded process in a shared-memory multiprocessor environment, each thread in the process can run concurrently on a separate processor, resulting in parallel execution, which is true simultaneous execution.\nFigure DetailsFigure 4.5.2: nogil multithreaded execution - htop view of CPU utilization (7 threads) Meanwhile, creating 7 threads on Cpython implementation gives really poor performance, as even though 7 threads exist, only 1 of them is running truly at a particular moment.\nFigure DetailsFigure 4.5.3: Cpython multithreaded execution - htop view of CPU utilization (7 threads) The shear performance improvement in nogil implementation over Cpython is mind blowing, making us wonder the highs MultiThreading in Python can touch -\nGraph DetailsGraph 4.5.1: Cpyton(GIL) vs nogil CPU Time taken by code to complete execution - comparison on multithreaded code As it is clearly visible, nogil completely outperforms the Cpython, even giving better performance in a single threaded program.\nGraph DetailsGraph 4.5.2: Cpyton(GIL) vs nogil ratio of improvement in execution time - (nogil / Cpython) As clearly shown above, nogil can achieve upto 5x better performance in multithreaded programs, and gives slightly better performance in single thread as well.\nMultiprocessing The multiprocessing library allows Python programs to start and communicate with Python sub-processes. This allows for parallelism because each sub-process has its own Python interpreter (i.e., there\u0026rsquo;s a GIL per-process).\nCommunication between processes is limited and generally more expensive than communication between threads.\nObjects generally need to be serialized or copied to shared memory. Starting a sub-process is also more expensive than starting a thread, especially with the \u0026ldquo;spawn\u0026rdquo; implementation.\nStarting a thread takes ~100 µs, while spawning a sub-process takes ~50 ms (50,000 µs) due to Python re-initialization.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Code 5.1: Multiprocessing demonstration in python import multiprocessing def fib(n): if n \u0026lt; 2: return 1 return fib(n-1) + fib(n-2) def test(): print(fib(50)) if __name__ == \u0026#34;__main__\u0026#34;: p1 = multiprocessing.Process(target=test) # creating processes p2 = multiprocessing.Process(target=test) p1.start() # starting processes p2.start() p1.join() # waiting for processes to finish p2.join() # both processes finished print(\u0026#34;Done!\u0026#34;) The multiprocessing library has a number of downsides.\nThe \u0026ldquo;fork\u0026rdquo; implementation is prone to deadlocks.\nMultiple processes are harder to debug than multiple threads. Sharing data between processes can be a performance bottleneck. For example, in PyTorch, which uses multiprocessing to parallelize data loading, copying Tensors to inter-process shared-memory is sometimes the most expensive operation in the data loading pipeline.\nAdditionally, many libraries don\u0026rsquo;t work well with multiple processes: for example CUDA doesn\u0026rsquo;t support \u0026ldquo;fork\u0026rdquo; and has limited IPC support.\nConclusion Seeing the level of improvement nogil is able to achieve, this is a big proof of a plausible world having a GIL-Free Python which increases multithreaded performance by many folds, having no negative impact on single threading. Even though there is a lot of work still pending, there is no doubt that the PEP 703 will be accepted in the near future, but only time will tell.\nReferences - discuss.python.org/t/a-fast-free-threading-python/27903\npython.org/ftp/python/contrib-09-Dec-1999/System/threading.README\nstackify.com/python-garbage-collection/\nLarry Hastings - Removing Python\u0026rsquo;s GIL: The Gilectomy - PyCon 2016\ndabeaz.blogspot.com/2011/08/inside-look-at-gil-removal-patch-of.html\npythoncapi.readthedocs.io/gilectomy.html\ngithub.com/larryhastings/gilectomy\ngithub.com/colesbury/nogil\ndocs.google.com/document/d/18CXhDb1ygxg-YXNBJNzfzZsDFosB5e6BfnXLlejd9l0\n","permalink":"https://aryaman.space/blog/breaking-boundaries/","summary":"Python faces challenges in fully exploiting the growing capabilities of modern hardware. As hardware continues to advance with more CPU cores, faster processors, and abundant memory, Python\u0026rsquo;s inherent design and execution model can often fall short in taking full advantage of these resources. Its single-threaded nature and certain architectural choices can result in suboptimal performance in scenarios where parallelism and hardware acceleration are vital. This limitation prompts developers to seek alternative solutions, such as integrating Python with external libraries, languages, or technologies, to overcome these hardware-related constraints.","title":"Breaking Boundaries"},{"content":"Sync vs Async What is synchronous programming?\nSynchronous programming is a programming paradigm in which operations are executed sequentially, one after the other. In this model, each operation waits for the previous one to complete before moving on to the next step. This sequential execution can lead to \u0026lsquo;blocking\u0026rsquo; operations, where certain tasks may take a significant amount of time to finish. These blocking operations can pause the entire program\u0026rsquo;s execution, forcing it to wait until the time-consuming task is done before it can proceed. Let\u0026rsquo;s understand it with an example:\n1 2 3 4 5 6 7 8 9 from time import sleep def foo(): print(\u0026#34;in foo\u0026#34;) sleep(10) # mimicking some blocking operation print(\u0026#34;end foo\u0026#34;) foo() print(\u0026#34;Termination of the program\u0026#34;) Here, you can observe that the program\u0026rsquo;s execution was strictly sequential, and this led to a halt in the entire program due to a blocking operation inside the foo() function, which took 10 seconds to complete. After this 10-second delay, the program continued its execution.\nThis situation isn\u0026rsquo;t ideal because these blocking operations could encompass a wide range of tasks, such as database queries or waiting for API responses, and they unnecessarily delay the execution of unrelated parts of the program. This is precisely where the concept of asynchronization becomes valuable.\nSo, what is asynchronization?\nIn computer programming, asynchrony encompasses events that occur independently of the primary program flow and methods for managing these events. In essence, asynchronization implies that you can proceed with executing other sections of code without the need to wait for a particular task to finish.\nThe first step in creating an asynchronous program is creating a coroutine.\nWhat\u0026rsquo;s a coroutine in Python? A coroutine is simply a wrapped version of a function that allows it to run asynchronously. I recommend using Python 3.7+ to follow this guide.\n1 2 3 4 5 6 import asyncio # importing async library async def main(): # defining a coroutine object print(\u0026#34;hello world\u0026#34;) print(main()) # Output - \u0026lt;coroutine object main at 0x7f013de5fd80\u0026gt; To create a coroutine object, you just need to create a function normally and add an async keyword in front of it. What it essentially does is create a wrapper around this function.\nSo when we call this function, it creates a coroutine object \u0026lt;coroutine object main at 0x7f013de5fd80\u0026gt;. This coroutine object works just like a function and can be executed. To execute a coroutine, you need to await it.\nWait, WHAT?! What\u0026rsquo;s awaiting? Let me show you the output of the above snippet of code.\nAs we have a coroutine object now, it does not work like a regular Python function.\nAsync Event-Loop Ok. No worries. Let me just await the main() function call, and then it should work, right?\n1 2 3 4 5 6 import asyncio async def main(): print(\u0026#34;hello world\u0026#34;) await main() Oh, no! It gave an error. So what\u0026rsquo;s the issue? The reason it\u0026rsquo;s not running our code because we haven\u0026rsquo;t created an event-loop.\nWhenever we create an asynchronous program in Python, we need to start/create an event loop. An event loop typically refers to the core mechanism used for managing and handling asynchronous or non-blocking operations. It\u0026rsquo;s a fundamental component of many asynchronous programming frameworks, such as asyncio, which we are using in our study. It is responsible for this simple async syntax we are seeing here in the backend. We must start an event loop in whatever thread we are running this asynchronous program in.\n1 2 3 4 5 6 import asyncio async def main(): print(\u0026#34;hello world\u0026#34;) asyncio.run(main()) Hence, to start an event loop, we use asyncio.run() and pass it a coroutine object, which acts as an entry point to the event loop.\nAwait keyword 1 2 3 4 5 6 7 8 9 10 11 12 import asyncio async def main(): print(\u0026#34;awaiting foo...\u0026#34;) await foo(\u0026#34;inside foo\u0026#34;) print(\u0026#34;finished awaiting foo\u0026#34;) async def foo(text): print(text) await asyncio.sleep(1) asyncio.run(main()) If we look inside the foo() function, the await keyword is required to run an object of coroutine, because if you just write something like asyncio.sleep(1) you are just creating a coroutine but not executing it.\nAnd we are allowed to use await because we are present inside an asynchronous function. So, if we try to see the output of the entire code, it would be as follows:\nWe can see that we awaited the foo() function call, and going inside the foo() function we waited for 1 sec and then the execution was back to the main() function.\nBut this is not entirely useful, as we can see that this seems to be just another way of writing a synchronous program as we are waiting for foo() to execute and then we go forward with the execution in the main() function.\nThe whole point of asynchronization is to run something else while we are waiting for some other part of the execution to finish which is not achieved here. This can be achieved by creating a task.\nasyncio Tasks 1 2 3 4 5 6 7 8 9 10 11 12 13 import asyncio async def main(): print(\u0026#34;awaiting foo...\u0026#34;) task = asyncio.create_task(foo(\u0026#34;inside foo\u0026#34;)) print(\u0026#34;finished awaiting foo\u0026#34;) async def foo(text): print(text) await asyncio.sleep(1) print(\u0026#34;Ending foo\u0026#34;) asyncio.run(main()) If we see the code above, we are creating a task using the create_task() function and passing a coroutine object to it. This essentially tells the program to run the task as soon as there is some kind of waiting in the main line of execution.\nAnalyzing the output, we can see that the execution of the main() function is completed and then the execution of foo() starts.\nVERY IMPORTANT RESULT\nYou can also see that the \u0026ldquo;Ending foo\u0026rdquo; never printed and the execution time of the program was only 0.132 sec. Weren\u0026rsquo;t we sleeping for 1 sec, so shouldn\u0026rsquo;t the execution time be at least 1 sec and \u0026ldquo;Ending foo\u0026rdquo; should be printed after? What happened here is when the foo() function started sleeping for 1 sec, the execution of the program was given back to the main() function, and as there weren\u0026rsquo;t any tasks left in the main() function, it terminated and as a result terminated the execution of foo() as well which never completed its execution.\nSo if I want to wait for the foo() to finish it\u0026rsquo;s execution before main() terminates I will need to await it, essentially preventing the main() function to terminate prior to the completion of the task foo().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import asyncio async def main(): print(\u0026#34;awaiting foo...\u0026#34;) task = asyncio.create_task(foo(\u0026#34;inside foo\u0026#34;)) print(\u0026#34;finished awaiting foo\u0026#34;) await task async def foo(text): print(text) await asyncio.sleep(1) print(\u0026#34;Ending foo\u0026#34;) asyncio.run(main()) Hence the output would be as follows (note the time taken for execution).\nMore Examples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import asyncio async def fetch_data(): print(\u0026#39;Start fetching\u0026#39;) await asyncio.sleep(2) # mimicking fetching data via some request print(\u0026#39;Done fetching\u0026#39;) return {\u0026#39;data\u0026#39;: 1} async def print_numbers(): for i in range(10): print(i) await asyncio.sleep(0.25) # mimicking waiting for a response async def main(): task1 = asyncio.create_task(fetch_data()) task2 = asyncio.create_task(print_numbers()) # awaiting task1 to be completed and then using # the value it returned value = await task1 print(value) await task2 asyncio.run(main()) Imagine the above scenario where I need to get the return value {'data': 1} from the fetch_data() coroutine executing via task1 in the main() function. Hence, I declare the task1 and print(task1). It would have worked in a synchronous environment. But, from what we have learned so far this is counterintuitive.\nAs we can see in the output of the above code, we never awaited for the task to be completed, hence we would never get the return value from the function (can relate to promises in JavaScript).\nHence the correct code should be as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import asyncio async def fetch_data(): print(\u0026#39;Start fetching\u0026#39;) await asyncio.sleep(2) # mimicking fetching data via some request print(\u0026#39;Done fetching\u0026#39;) return {\u0026#39;data\u0026#39;: 1} async def print_numbers(): for i in range(10): print(i) await asyncio.sleep(0.25) # mimicking waiting for a response async def main(): task1 = asyncio.create_task(fetch_data()) task2 = asyncio.create_task(print_numbers()) # awaiting for task1 to be completed and then using # the value it returned value = await task1 print(value) await task2 asyncio.run(main()) As we can see in the output, because we awaited for task1 to finish, we can get the value returned by it and print it later. We can further see that as fetch_data() was waiting for 2 sec, for loop in print_numbers() executed 8 times (~2 sec) and gave a chance back to fetch_data() as it\u0026rsquo;s waiting time had ended.\nConclusion When we write async def func_name(), what we are doing is wrapping another function with an asynchronous version of it. That version is the Coroutine. To run the coroutine, you must await it in some way or add it to the event loop. You can add it to the event loop by creating a task (asyncio.create_task(func_name())). This allows you to start multiple coroutines concurrently. Finally, to start your asynchronous program, you must create an event loop, which can be done by asyncio.run(driver_func()) and passing an entry point coroutine, which is generally the driver function of your program (for example, the main function).\nOne thing to note is that at a time only a single execution is happening, and it is not to be confused with parallel processing or threading.\nReferences deepsource.com/glossary/synchronous-programming\nen.wikipedia.org/wiki/Asynchrony_(computer_programming)\nasyncio.run/\ndocs.python.org/3/library/asyncio-task.htm\ndocs.python.org/3/library/asyncio-eventloop.html\nyoutube.com/@TechWithTim\n","permalink":"https://aryaman.space/blog/asynchronous-programming-in-python/","summary":"Sync vs Async What is synchronous programming?\nSynchronous programming is a programming paradigm in which operations are executed sequentially, one after the other. In this model, each operation waits for the previous one to complete before moving on to the next step. This sequential execution can lead to \u0026lsquo;blocking\u0026rsquo; operations, where certain tasks may take a significant amount of time to finish. These blocking operations can pause the entire program\u0026rsquo;s execution, forcing it to wait until the time-consuming task is done before it can proceed.","title":"Asynchronous Programming in Python"},{"content":"Lessons learned from my internship at a government firm Introduction Embarking on an internship journey within a corporate government firm can be exciting and challenging. I was amazed by the valuable lessons and experiences that awaited me as I delved into this professional realm. In this article, I will share the key insights I gained during my internship, from the importance of collaboration to the significance of clean code practices. Join me as we uncover the secrets to thriving in this dynamic and rewarding environment.\nKey Learnings Collaboration: The Key to Success One of the most remarkable aspects of my internship was the camaraderie among the employees. The friendly and helpful nature of my colleagues fostered an environment where no questions went unanswered. Whenever I encountered challenges, reaching out to others working on similar projects proved a game-changer. Their invaluable insights and experiences solved my problems faster and nurtured a sense of community within the firm.\nBalancing Transparency and Timeliness In the corporate government setting, I quickly realized that requirements were ever-evolving. As soon as one set of tasks seemed complete, a new wave of responsibilities would follow. It became evident that being strategic in communication was vital. While my manager was cordial, it became apparent that politeness often hinged on meeting deadlines. Knowing when and how to communicate with superiors became an essential skill in navigating the corporate landscape.\nEmbracing Productivity: Deadlines as Catalysts At first, the daily deadlines at the firm may seem like a lot, but they actually have a positive impact. They act as motivators, encouraging individuals to work harder and concentrate better. Achieving these daily goals not only ensures that projects stay on schedule but also gives the team a sense of satisfaction.\nWriting Code That Stands the Test of Time Perhaps one of the most enduring lessons was the value of writing clean and organized code. Contrary to the belief that functional code is enough, I discovered that clean code is the foundation for sustainable success. By cultivating the habit of writing elegant code, not only did I make future changes and references more manageable for myself but also ensured that my work would be comprehensible and beneficial for future developers.\n💡 freeCodeCamp - How to Write Clean Code? The Path to Clarity: Asking the Right Questions Clarity was the cornerstone of my success during the internship. When confronted with uncertainty about a task or project, I learned the importance of asking questions. The more I inquired about the requirements and sought to understand the project\u0026rsquo;s nuances, the clearer my path to successful implementation became.\n💡 One effective approach is to create a DFD (Data Flow Diagram) for your current project. Additionally, you can develop an ERD (Entity Relationship Diagram) specific to your project. It is also useful to have a Validation Sheet for managing forms, as this simplifies the coding process. When I began my internship, I took on the application assigned to me with a user-centric approach, without realizing its designation-based nature. Initially, the login I was using seemed to be working fine. However, during the testing phase, problems arose, and the application ceased to function.\nConclusion Armed with these newfound skills, I am now better prepared to navigate future challenges and make a lasting impact in the professional world. The journey may have been intense, but the rewards of personal and professional growth have been immeasurable. But the main question still remains unanswered.\nShould I be pursuing a full-time job in the corporate sector? If the office culture promotes work-life balance, offers flexible working hours, and fosters personal growth, then I would choose it over a high-paying job. These factors are crucial for maintaining mental peace and overall well-being.\n","permalink":"https://aryaman.space/blog/finding-success-in-a-dynamic-environment/","summary":"Lessons learned from my internship at a government firm Introduction Embarking on an internship journey within a corporate government firm can be exciting and challenging. I was amazed by the valuable lessons and experiences that awaited me as I delved into this professional realm. In this article, I will share the key insights I gained during my internship, from the importance of collaboration to the significance of clean code practices. Join me as we uncover the secrets to thriving in this dynamic and rewarding environment.","title":"Finding Success in a Dynamic Environment"},{"content":"What on earth are decorators??? Decorators are essentially single reusable functions that take a \u0026ldquo;function\u0026rdquo; as input and return a modified version of it. Decorators are just a bit different from regular functions because they wrap the \u0026ldquo;input function\u0026rdquo; to extend its functionality without modifying it.\nWhat does wrapping mean?\n1 2 3 4 5 6 import time start_time = time.time() **call your function** #calling your function end_time = time.time() print(\u0026#34;Time Taken = \u0026#34;, end_time-start_time) Here you can see that your function call is being \u0026ldquo;wrapped\u0026rdquo; between lines of code.\nHow are functions getting into another function - First Class Citizens A first-class citizen (also type, object, entity, or value) in a given programming language is an entity that supports all the operations generally available to other entities. These operations typically include being passed as an argument, returned from a function, and assigned to a variable.\nApart from data types like integers, floating type, strings etc., functions and classes are also First Class Citizens in Python.\n1 2 3 4 5 def compose(f,g,x): return f(g(x)) \u0026gt;\u0026gt; compose(print, len, \u0026#34;abc\u0026#34;) #passing print, len functions in function-call 3 In the above example, we can see that passing functions into another function as an argument is no biggie as functions are First Class Citizens in Python (and so are classes).\nYou might wonder what does this have to do with decorators?\nOnly because functions are first-class citizens do decorators work. The existence of decorators is based on the fact that Python allows functions to be supplied as arguments and returned from functions, as you will see in the following sections of this article.\nMoving towards Decorators Decorators start with an @ symbol.\nThere are 3 types of decorators in total -\nFunction decorators (decorators wrapping functions)\nClass decorators (decorators wrapping classes)\nMethod decorators (decorators wrapping methods ~ functions belonging to an object)\nLet us consider an example-\n1 2 3 4 5 6 7 8 9 def wrapper(a): def x(*args, **kwargs): print(\u0026#34;start\u0026#34;) a(*args, **kwargs) print(\u0026#34;end\u0026#34;) return x def print_sum(a,b,c): print(a+b+c) What are we doing here?\nCreated a function named \u0026ldquo;print_sum\u0026rdquo; which takes 3 input arguments and prints their sum\nCreating a nested function named \u0026ldquo;wrapper\u0026rdquo; which takes a function as an input and returns a function \u0026ldquo;x\u0026rdquo; which wraps that function call of its input function between 2 lines of print statement\n(*args, **kwargs) just takes all the arguments presented during the function call, it doesn\u0026rsquo;t have any other role here\nPlaying around with the above code -\n1 2 \u0026gt;\u0026gt;\u0026gt; print(wrapper(print_sum)) \u0026lt;function wrapper.\u0026lt;locals\u0026gt;.x at 0x000001EB1087B0D0\u0026gt; As we can see, I passed the \u0026ldquo;print_sum\u0026rdquo; function into the wrapper function \u0026ldquo;wrapper\u0026rdquo;, which returns us with another function.\nLet us try assigning this function to a variable and then calling it -\n1 2 3 4 5 6 7 \u0026gt;\u0026gt;\u0026gt; print_sum(3,4,1) #normal print_sum function 8 \u0026gt;\u0026gt;\u0026gt; wrapped_fun = wrapper(print_sum) #print_sum function wrapped in wrapper \u0026gt;\u0026gt;\u0026gt; wrapped_fun(3,4,1) #calling the new function start 8 end What can we deduce from the above execution?\nJust calling \u0026ldquo;print_sum(3, 4, 1)\u0026rdquo; gives us an output of 8. But when wrapped around by the wrapper function the final output changes\nThe wrapper function prints \u0026ldquo;start\u0026rdquo;, calls the \u0026ldquo;print_sum\u0026rdquo; with arguments (3, 4, 1) and then prints \u0026ldquo;end\u0026rdquo; ~ all happening as stated inside the nested wrapper function\nNow instead of writing throw-away code like above, we can use @ symbol to wrap the function like so -\n1 2 3 4 5 6 7 8 9 10 def wrapper(a): def x(*args, **kwargs): print(\u0026#34;start\u0026#34;) a(*args, **kwargs) print(\u0026#34;end\u0026#34;) return x @wrapper #decorator def print_sum(a,b,c): print(a+b+c) Now simply calling the \u0026ldquo;print_sum\u0026rdquo; function gives us the wrapped output, as desired.\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; print_sum(4,3,1) start 8 end Looking into a more practical example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import math import time def calc_time(func): def x(*args, **kwargs): time_start = time.time() func(*args, **kwargs) time_end = time.time() print(\u0026#34;\\nTime Taken = \u0026#34;, time_end - time_start) return x @calc_time def primeFactors(n): while n % 2 == 0: print(2, end=\u0026#34; \u0026#34;) n = n / 2 for i in range(3,int(math.sqrt(n))+1,2): while n % i== 0: print(i, end= \u0026#34; \u0026#34;) n = n / i if n \u0026gt; 2: print(n, end=\u0026#34; \u0026#34;) Here we have created a function to print all the prime factors of a number and give its running time.\n1 2 3 \u0026gt;\u0026gt;\u0026gt; primeFactors(99999999900) 2 2 3 3 3 3 5 5 37 333667.0 Time Taken = 0.021126508712768555 We get the prime factors of the number and also its running time.\nDecorators with arguments Decorators with arguments have a bit different syntax. The decorator with arguments should return a function that will take a function and return another function ~ basically, we are to create a function which returns a decorator.\nLet\u0026rsquo;s see it with an example -\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import math import time def main_wrapper(num): #function which takes an argument and returns the wrapper function def calc_time(func): def x(*args, **kwargs): time_start = time.time() func(*args, **kwargs) time_end = time.time() print(\u0026#34;\\nTime Taken = \u0026#34;, time_end - time_start) print(num) #using the input argument here return x return calc_time #returning the wrapper function @main_wrapper(2) #wrapper function with an argument def primeFactors(n): while n % 2 == 0: print(2, end=\u0026#34; \u0026#34;) n = n / 2 for i in range(3,int(math.sqrt(n))+1,2): while n % i== 0: print(i, end= \u0026#34; \u0026#34;) n = n / i if n \u0026gt; 2: print(n, end=\u0026#34; \u0026#34;) \u0026gt;\u0026gt;\u0026gt; primeFactors(99999999900) 2 2 3 3 3 3 5 5 37 333667.0 Time Taken = 0.02055978775024414 2 Conclusion In conclusion, decorators in Python provide a powerful mechanism to extend the functionality of functions and classes without modifying their code directly. By using decorators, you can encapsulate common behaviours, and add logging, timing, authentication, or any other cross-cutting concerns to your functions or classes.\nDecorators leverage the concept of first-class citizens in Python, treating functions and classes as objects that can be passed as arguments, returned from other functions, and assigned to variables. This flexibility allows decorators to wrap and modify the behaviour of functions or classes seamlessly.\nOverall, decorators are a powerful feature of Python that allows you to easily extend the behaviour of functions and classes, making your code more modular, flexible, and expressive. By mastering decorators, you can unlock a wide range of possibilities and improve the quality of your Python projects.\nReferences geeksforgeeks.org/decorators-with-parameters-in-python/\nyoutube.com/@socratica\nstackoverflow.com/questions/5929107/decorators-with-parameters\nen.wikipedia.org/wiki/First-class_citizen\n","permalink":"https://aryaman.space/blog/decorators-in-python/","summary":"What on earth are decorators??? Decorators are essentially single reusable functions that take a \u0026ldquo;function\u0026rdquo; as input and return a modified version of it. Decorators are just a bit different from regular functions because they wrap the \u0026ldquo;input function\u0026rdquo; to extend its functionality without modifying it.\nWhat does wrapping mean?\n1 2 3 4 5 6 import time start_time = time.time() **call your function** #calling your function end_time = time.time() print(\u0026#34;Time Taken = \u0026#34;, end_time-start_time) Here you can see that your function call is being \u0026ldquo;wrapped\u0026rdquo; between lines of code.","title":"Decorators in Python"},{"content":"\nI am Aryaman.\nAppreciate you for being here.\nWhether I\u0026rsquo;m dissecting massive SaaS architectures or tinkering with hardware, I thrive on diving deep into the details. Sharing what I learn feels super empowering, and it\u0026rsquo;s all fueled by my constant hunt for new challenges.\nThis little corner of the internet has been my secret spot for ideas, projects, and favorite reads — until now. I\u0026rsquo;m really excited to have you along for the ride!\nNext up on my radar? Deep Learning and Competitive Programming\nAlways happy to talk on Twitter and LinkedIn.\nYou can check out my work on GitHub\n","permalink":"https://aryaman.space/about/","summary":"About Aryaman Gupta","title":"About Me"},{"content":"I\u0026rsquo;ve been diving into the world of research and white papers. So far, I\u0026rsquo;ve checked out these awesome articles -\nAI/ML Chain-of-Thought Prompting MemGPT: Towards LLMs as Operating Systems Camelot: Towards Large Language Models with Training-Free Consolidated Associative Memory Named Entity Recognition with Bidirectional LSTM-CNNs Attention Is All You Need Quantum Computing Partial Loopholes Free Device-Independent Quantum Random Number Generator Using IBM\u0026rsquo;s Quantum Computers Experiments with IBM Quantum Devices for Random Number Generation and String Matching Quantum Supremacy Using a Programmable Superconducting Processor Quantum Computation by David P. DiVencenzo Software Dev WTF: The Who to Follow Service at Twitter ","permalink":"https://aryaman.space/papershelf/","summary":"I\u0026rsquo;ve been diving into the world of research and white papers. So far, I\u0026rsquo;ve checked out these awesome articles -\nAI/ML Chain-of-Thought Prompting MemGPT: Towards LLMs as Operating Systems Camelot: Towards Large Language Models with Training-Free Consolidated Associative Memory Named Entity Recognition with Bidirectional LSTM-CNNs Attention Is All You Need Quantum Computing Partial Loopholes Free Device-Independent Quantum Random Number Generator Using IBM\u0026rsquo;s Quantum Computers Experiments with IBM Quantum Devices for Random Number Generation and String Matching Quantum Supremacy Using a Programmable Superconducting Processor Quantum Computation by David P.","title":"Papershelf"}]